# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UgYD4UBLOm0OwJzwrd2_heNJ6mtXplez
"""

import torch
import numpy

class CustomInverse(torch.nn.Module):
    def forward(self, x):
        return x - 1
        #return torch.inverse(x) + x

x = torch.randn(1, 1)
print('x:', x)

model = CustomInverse()
y = model(x)
print('y:', y)

import io

# Export model to ONNX a variable f tipo archivo en memoria
f = io.BytesIO()
torch.onnx.export(CustomInverse(), (x,), f)

# Export model to ONNX archivo en carpeta
filename = './custominverse.onnx'
torch.onnx.export(CustomInverse(), (x,), filename)

model = CustomInverse()
pt_outputs = model(x)
print('o:', pt_outputs)

f.getvalue()

pip install onnxruntime

import onnxruntime    # ya no existe en python

# Run the exported model with ONNX Runtime
ort_sess = onnxruntime.InferenceSession(f.getvalue())
ort_inputs = dict((ort_sess.get_inputs()[i].name, input.cpu().numpy()) for i, input in enumerate((x,)))
ort_outputs = ort_sess.run(None, ort_inputs)

# Validate PyTorch and ONNX Runtime results
numpy.testing.assert_allclose(pt_outputs.cpu().numpy(), ort_outputs[0], rtol=1e-03, atol=1e-05)
print(ort_outputs)

# Run the exported model with ONNX Runtime
ort_sess = onnxruntime.InferenceSession(filename)
ort_inputs = dict((ort_sess.get_inputs()[i].name, input.cpu().numpy()) for i, input in enumerate((x,)))
ort_outputs = ort_sess.run(None, ort_inputs)

# Validate PyTorch and ONNX Runtime results
numpy.testing.assert_allclose(pt_outputs.cpu().numpy(), ort_outputs[0], rtol=1e-03, atol=1e-05)
print(ort_outputs)

ort_sess.get_outputs()[0].name, ort_sess.get_outputs()[0].type, ort_sess.get_outputs()[0].shape

ort_sess.get_inputs()[0].name, ort_sess.get_inputs()[0].type, ort_sess.get_inputs()[0].shape

print('x:', x)
for i, inp in enumerate((x,)):
  print(i, inp)

ort_inputs

ort_inputs

ort_outputs